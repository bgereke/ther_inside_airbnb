---
title: "The R Inside Airbnb"
output:
  html_notebook: default
  pdf_document: default
---

Inside Airbnb is an anti-airbnb activist website that hosts airbnb listing availability data for many cities worldwide. The data is scraped from the airbnb website every few days or so, and represents listing information that was live at the time of the scrape. The goal of this notebook will be to explore a subset of the data and answer two primary questions: First, what factors drive host pricing decisions? Second, what factors drive customer booking decisions?  

## Accessing the data

To access the data, I began by scraping the data file URLs from the Inside Airbnb website, and used those endpoints to construct a PostgreSQL database from the hosted csv files. Let's source that library now: 

```{r}
source("inside_airbnb_to_postgres.R")
```

Using this library, all of the csv file URLs can be organinzed into a table by calling:

```{r}
file_urls <- scrape_inside_airbnb()
head(file_urls)
```

The full dataset consists of 7 table types, each with `r nrow(file_urls)` files. Let's see what the most scraped countries, states and cities are.

```{r}
scrapes_by_country <- file_urls %>% 
  group_by(url_country) %>% 
  summarize(num_scrapes = n()) %>% 
  arrange(desc(num_scrapes)) %>% collect

options(repr.plot.height = 600)
ggplot(data = scrapes_by_country, aes(x = reorder(url_country, -num_scrapes), y = num_scrapes)) + 
  geom_bar(stat = "identity") +
  xlab('country') + ylab('# scrapes') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

scrapes_by_state <- file_urls %>% 
  filter(url_country == "united-states") %>%
  group_by(url_state) %>% 
  summarize(num_scrapes = n()) %>% 
  arrange(desc(num_scrapes)) %>% collect

options(repr.plot.height = 600)
ggplot(data = scrapes_by_state, aes(x = reorder(url_state, -num_scrapes), y = num_scrapes)) + 
  geom_bar(stat = "identity") +
  xlab('state') + ylab('# scrapes') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

scrapes_by_city <- file_urls %>% 
  filter(url_country == "united-states" & url_state == "ca" | url_state == "or") %>%
  group_by(url_city, url_state) %>% 
  summarize(num_scrapes = n()) %>% 
  arrange(desc(num_scrapes)) %>% collect

options(repr.plot.height = 600)
ggplot(data = scrapes_by_city, aes(x = reorder(url_city, -num_scrapes), y = num_scrapes, fill = url_state)) + 
  geom_bar(stat = "identity") +
  xlab('city') + ylab('# scrapes') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

```

## Building a database

As we can see, San Francisco and Portland are two of the most scraped cities in the dataset. From here on, we'll restrict the analysis to these two cities. Let's go ahead and connect to the PostgreSQL database:

```{r}
library(DBI)
con <- DBI::dbConnect(odbc::odbc(), Driver = "PostgreSQL ODBC Driver(ANSI)", 
    Server = "localhost", Database = "inside_airbnb", UID = rstudioapi::askForPassword("Database user"), 
    PWD = rstudioapi::askForPassword("Database password"), Port = 5432)
tmp <- dbExecute(con, "SET CLIENT_ENCODING TO 'utf8'")
```

We can populate the database with any list of cities and table types by running the code chunk below; however, downloading and converting the files takes some time, so this was done ahead of time.

```{r}
cities <- list("san-francisco", "portland")
table_names <- list("summary_listings", "calendar", "detailed_review", "detailed_listings", "summary_review", "neighbourhood")
# num_rows_affected <- build_pgdb(con, cities = cities, table_names = table_names)
```

The two main tables of interest are "calendar" which contains the listing prices and availabilities for the upcoming year, and "detailed_listings" which contains some features for each listing (e.g., host, location, property details, etc.). Let's run some simple queries to see how many listings we have for each city:

```{sql, connection = con, output.var="num_listings"}
SELECT smart_location, COUNT(DISTINCT id)
FROM detailed_listings
GROUP BY smart_location;
```
```{r}
num_listings
```
```{r}
sql_count_cols <- "SELECT COUNT(*) FROM information_schema.columns 
                  WHERE table_name = 'detailed_listings'"
num_features <- as.integer(dbGetQuery(con, sql_count_cols)$count)
sql_count_records <- "SELECT COUNT(*) FROM calendar"
num_records <- as.integer(dbGetQuery(con, sql_count_records)$count)
```

Uh oh... it appears the cities weren't coded very well. Overall, "detailed_listings" has `r num_features` features, and many of them either aren't coded very well, or contain redundant or irrelevant information for our purposes. Also, the data is pretty big... the "calendar" table has `r num_records` records across all listings and dates. We're going to need to do some data cleansing and filtering to obtain something more reasonable to work with.  

## Cleaning and preprocessing
Let's go ahead and read in "detailed listings" and decided which features are worth keeping.
```{r}
detailed_listings <- dbReadTable(con, "detailed_listings")
head(detailed_listings)
```
A number of columns are mostly missing data (i.e., NA's), have only one distinct value, or are URL's. We can drop those, and some other features we may not care about such as the scrape id, etc. We'll drop all but one of the city features and factorize the remaining feature. We also recode all features with mosly t's ad f's to boolean. Many of the features are long text descriptions. While we could apply some NLP or sentiment analysis to these entries to construct new features, we'll keep things simple and convert these to numeric values corresponding to the length of the entry. This should greatly reduce the size of the data but also throws away a good deal of information. We can always return to beef up the feature count here. Also, some of the URL's link to important data such as pictures of the rental units. We'll ignore those for now as well, but may return to them in the future. 

```{r}
source("data_cleaning_utils.R")

detailed_listings <- detailed_listings %>%
  drop_junk %>%
  factorize_booleans %>%
  factorize_city(city_names = c("Portland","San Francisco"), state_names = c("OR","CA")) %>%
  factorize_amenities %>%
  factorize_host_verifications %>%
  factorize_license %>%
  factorize_simple_cols %>%
  factorize_cancellation_policy %>%
  refactor_property_types %>%
  refactor_bed_type %>%
  convert_dates %>%
  host_rates_to_numeric %>%
  text_to_numeric %>%
  process_missing 
  
  
head(detailed_listings)
```

## Exploratory Data Analysis

Let's do some simple eda to verify the cleaning steps, and get a sense how the variables are distribtued and correlated.

```{r}
suppressMessages(library(DataExplorer))
suppressWarnings({
  plot_intro(detailed_listings)
  plot_bar(detailed_listings)
  plot_histogram(detailed_listings)
  plot_correlation(detailed_listings, type = 'continuous', cor_args = list("method" = "spearman"))
  plot_correlation(detailed_listings, type = 'discrete', cor_args = list("method" = "spearman"))
  # numeric_cols <- sapply(detailed_listings, function(x) is.numeric(x))
  # plot_prcomp(detailed_listings[,numeric_cols], variance_cap = 0.5, maxcat = 10L)
  # plot_qq(detailed_listings)
  plot_scatterplot(detailed_listings, by = "price")
})
 
```
Now let's read in the calendar table and performing an inner join with some of the cleaned detailed listings features from above that we might want to include:
```{r}
suppressMessages(library("timeDate"))

calendar <- dbReadTable(con, "calendar")
calendar$date <- as.Date(calendar$date)
calendar$available <- as.factor(calendar$available)
features <- c('accomodates', 'cleaning_fee', 
              'extra_people', 'number_of_reviews', 
              'bathrooms', 'security_deposit', 
              'neighbourhood_cleansed', 'calculated_host_listings',
              'smart_location')

full_df <- calendar %>%
  inner_join(detailed_listings[,c("id", features)], by = c("listing_id" = "id")) %>%
  select(-c('adjusted_price')) %>%
  mutate(num_date = as.numeric(date),
         weekday = as.factor(weekdays(date)),
         is_holiday = as.factor(isHoliday(as.timeDate(date))),
         listing_id = as.factor(listing_id),
         neighbourhood_cleansed = as.factor(neighbourhood_cleansed))
full_df <- full_df[complete.cases(full_df),]

rm(detailed_listings, calendar)
head(full_df)
```
Now we can do a slightly more detailed eda.
```{r}
suppressMessages(library(DataExplorer))
suppressWarnings({
  plot_histogram(full_df)
  plot_correlation(full_df, cor_args = list("method" = "spearman"))
})

num_listings <- 5
sample_df <- full_df %>%
  filter(listing_id %in% sample(unique(listing_id), num_listings, replace = FALSE)) %>%
  group_by(listing_id) #%>%
  # mutate(price = price / median(price))
options(repr.plot.height = 600)
ggplot(data = sample_df, aes(x = date, y = price, group = listing_id, color = listing_id)) + 
  geom_line(size = 1) +
  xlab('date') + ylab('price (dollars or normalized)')
```
From these visualizations, we can tell that price dynamics vary substantially from listing to listing and many appear non-stationary. Predictions will be better if we can identify groups of similarly behaving listings and model them separately. We'll try clustering the time series' power spectra:
```{r}
suppressMessages({
  library(umap)
  library(dtwclust)
})
  

#get complete calendars
calendars <- full_df %>%
  group_by(listing_id) %>%
  mutate(price = log(price + 1)) %>%
  mutate(price = price - mean(price)) %>%
  ungroup %>%
  pivot_wider(id_cols = listing_id, 
              values_from = price, 
              names_from = date, 
              values_fill = list(price = 0)) 
calendars <- calendars[complete.cases(calendars),]
calendars <- calendars[,colSums(is.na(calendars)) == 0]
calendars <- as.matrix(calendars[,2:366])
calendars <- calendars[rowSums(calendars == 0) < ncol(calendars),]

#get multitapered power spectra
calendars_df <- as.data.frame(t(calendars))
specs <- map(calendars_df, ts) %>%
  map(spectrum, plot = FALSE, span = 10) %>%
  map(purrr::pluck, "spec")
freqs <- map(calendars_df, ts) %>%
  map(spectrum, plot = FALSE, span = 10) %>%
  map(purrr::pluck, "freq")
specs <- log(t(matrix(replace_na(unlist(specs),0), ncol = ncol(calendars_df))))
# specs <- apply(specs, 2, function(x) x / sum(x))

#clustering via time series clustering approach (pam w/ dtw distance)
spec_clust <- tsclust(specs, 
                 type = "tadpole",
                 k = 6,
                 control = tadpole_control(dc = 0.1, window.size = 10))
cal_clust <- tsclust(calendars, 
                 type = "tadpole",
                 k = 6,
                 control = tadpole_control(dc = 0.1, window.size = 4))
plot(spec_clust, type = 'sc', linetype = 'solid')
plot(cal_clust, type = 'sc', linetype = 'solid')

#dimensionality reduction (pca or umap)
# comps <- prcomp(specs, scale = TRUE, center = TRUE)
umapped_specs <- umap(specs)
umapped_cals <- umap(calendars)
umap_df <- data.frame('umap_dimension_1' = c(umapped_specs$layout[,1], 
                                             umapped_cals$layout[,1]),
                      'umap_dimension_2' = c(umapped_specs$layout[,2], 
                                             umapped_cals$layout[,2]),
                      'clust' = factor(c(spec_clust@cluster, 
                                         cal_clust@cluster)),
                      'type' = factor(rep(c("spectra","time_series"), each = nrow(specs))))
colors <- c("#FFDB6D", "#C4961A", "#D16103", "#C3D7A4", "#52854C", "#293352", "#F4EDCA", "#4E84C4")
plt <- ggplot(data = umap_df, aes(x = umap_dimension_1, y = umap_dimension_2, group = clust, color = clust)) + 
  geom_point(size = 1.5) +
  scale_colour_manual(values = colors) + 
  facet_grid(.~type) +
  coord_fixed() +  
  xlim(-25, 25) + ylim(-25, 25)
suppressWarnings(print(plt))

```

## Predicting Advertised Price
Our goal will now be to predict listing price across the calendar year using features chosen based on the above eda. 
```{r}
suppressMessages({
  library(xgboost)
  library(glmnet)
  library(mlr)
  library(Ckmeans.1d.dp)
})

#select features and target 
features <- c("available","minimum_nights",
              "maximum_nights","accomodates",
              "cleaning_fee","extra_people",
              "number_of_reviews","bathrooms",
              "security_deposit","calculated_host_listings",
              "smart_location", "weekday", 
              "is_holiday", "num_date")
full_df$log_price <- (full_df$price + 1)

#breakup training and test sets
test_frac <- 0.15
train_listings <- c()
for (city in levels(full_df$smart_location)){
  city_listings <- unique(full_df$listing_id[full_df$smart_location == city])
  num_city_listings <- length(city_listings)
  train_listings <- c(train_listings,
                     sample(city_listings,
                            floor(test_frac*num_city_listings),
                            replace = FALSE))
}

#prepare training and test sets for xgboost
train_idx <- as.integer(full_df$listing_id) %in% train_listings
test_idx <- !(as.integer(full_df$listing_id) %in% train_listings)
xgtrain <- xgb.DMatrix(data = makeX(full_df[train_idx, features],
                                    sparse = TRUE),
                       label = full_df$log_price[train_idx])

xgtest <- xgb.DMatrix(data = makeX(full_df[test_idx, features],
                                   sparse = TRUE),
                      label = full_df$log_price[test_idx])

#xgboost cross validation parameters
cvparams <- list(booster = "gbtree",
                 objective = "reg:squarederror",
                 eta=0.3,
                 gamma=0,
                 max_depth=10)

#k-fold cross-validated xgboost
#take folds by sampling listings as above
num_folds <- 5
folds <- vector("list", num_folds)
for (fold in 1:num_folds){
  fold_indices <- c()
  for (city in levels(full_df$smart_location)){
    city_listings <- unique(full_df$listing_id[full_df$smart_location[train_idx] == city])
    num_city_listings <- length(city_listings)
    sample_listings <-   sample(city_listings,
                         floor(test_frac*num_city_listings),
                         replace = FALSE)
    fold_indices <- c(fold_indices, which(full_df$listing_id[train_idx] %in% sample_listings))
  }
  folds[[fold]] <- fold_indices
}

xgbcv <- xgb.cv(params = cvparams,
                data = xgtrain,
                nrounds = 300,
                folds = folds,
                showsd = T,
                print_every_n = 10,
                verbose = FALSE,
                early_stopping_rounds = 20,
                maximize = F)

xgmod <- xgb.train(data = xgtrain,
                  params = cvparams,
                  nrounds = xgbcv$best_iteration,
                  print_every_n = 10,
                  verbose = FALSE,
                  watchlist = list(val= xgtest,train = xgtrain))

xgpred <- predict(xgmod, xgtest)

# hyper paramater tuning (Not Run)
#
# train_task <- makeRegrTask(id = "1", 
#                            data = full_df[!(as.integer(full_df$listing_id) %in% test_listings),][,c(features,'log_price')], 
#                            target = "log_price")
# test_task <- makeRegrTask(id = "2", 
#                           data = full_df[as.integer(full_df$listing_id) %in% test_listings,][,c(features,'log_price')], 
#                           target = "log_price")
# train_task <- createDummyFeatures (obj = train_task)
# test_task <- createDummyFeatures (obj = test_task)
# xglearner <- makeLearner("regr.xgboost",
#                          predict.type = "response")
# xglearner$par.vals <- list(objective="reg:squarederror", 
#                            eval_metric="error", 
#                            nrounds=300L, 
#                            eta=0.3)
# tune_params <- makeParamSet(makeDiscreteParam("booster", values = c("gbtree","gblinear")), 
#                        makeIntegerParam("max_depth",lower = 5L, upper = 15L), 
#                        makeNumericParam("min_child_weight",lower = 1L,upper = 10L), 
#                        makeNumericParam("subsample",lower = 0.5,upper = 1), 
#                        makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
# rdesc <- makeResampleDesc("CV", iters = 5L)
# ctrl <- makeTuneControlRandom(maxit = 10L)
# xgtune <- tuneParams(learner = xglearner, 
#                      task = train_task, 
#                      resampling = rdesc, 
#                      par.set = tune_params, 
#                      control = ctrl, 
#                      show.info = T)
# best_tune <- setHyperPars(xglearner, par.vals = xgtune$x)
# xgmodel <- train(learner = best_tune, task = train_task)
# xgpred <- predict(xgmodel, test_task)

r_squared <- 1 - sum((full_df$log_price[test_idx] - xgpred)^2) / sum((full_df$log_price[test_idx] - mean(full_df$log_price[test_idx]))^2)

importance <- xgb.importance(feature_names = colnames(xgtrain), model = xgmod)
xgb.ggplot.importance(importance_matrix = importance)
xgb.plot.shap(data = makeX(full_df[test_idx, features], sparse = TRUE), 
              model = xgmod, 
              top_n = 12, 
              n_col = 3, 
              plot_loess = TRUE)
```
The model has an out-of-sample R^2^ of `r r_squared`. Not terrible.

## Predicting Occupancy
We'll now repeat a similar analysis predicting listing availability over the calendar year.
```{r}
suppressMessages({
  library(xgboost)
  library(glmnet)
  library(mlr)
  library(Ckmeans.1d.dp)
  library(caret)
  library(pROC)
})

#select features and target 
features <- c("price","minimum_nights",
              "maximum_nights","accomodates",
              "cleaning_fee","extra_people",
              "number_of_reviews","bathrooms",
              "security_deposit","calculated_host_listings",
              "smart_location", "weekday", 
              "is_holiday", "num_date")

#breakup training and test sets
test_frac <- 0.15
train_listings <- c()
for (city in levels(full_df$smart_location)){
  city_listings <- unique(full_df$listing_id[full_df$smart_location == city])
  num_city_listings <- length(city_listings)
  train_listings <- c(train_listings,
                     sample(city_listings,
                            floor(test_frac*num_city_listings),
                            replace = FALSE))
}

#prepare training and test sets for xgboost
train_idx <- as.integer(full_df$listing_id) %in% train_listings
test_idx <- !(as.integer(full_df$listing_id) %in% train_listings)
xgtrain <- xgb.DMatrix(data = makeX(full_df[train_idx, features],
                                    sparse = TRUE),
                       label = as.numeric(full_df$available[train_idx] == 't'))

xgtest <- xgb.DMatrix(data = makeX(full_df[test_idx, features],
                                   sparse = TRUE),
                      label = as.numeric(full_df$available[test_idx] == 't'))

#xgboost cross validation parameters
cvparams <- list(booster = "gbtree",
                 objective = "binary:logistic",
                 eta=0.3,
                 gamma=0,
                 max_depth=10)

#k-fold cross-validated xgboost
#take folds by sampling listings as above
num_folds <- 5
folds <- vector("list", num_folds)
for (fold in 1:num_folds){
  fold_indices <- c()
  for (city in levels(full_df$smart_location)){
    city_listings <- unique(full_df$listing_id[full_df$smart_location[train_idx] == city])
    num_city_listings <- length(city_listings)
    sample_listings <-   sample(city_listings,
                         floor(test_frac*num_city_listings),
                         replace = FALSE)
    fold_indices <- c(fold_indices, which(full_df$listing_id[train_idx] %in% sample_listings))
  }
  folds[[fold]] <- fold_indices
}

xgbcv <- xgb.cv(params = cvparams,
                data = xgtrain,
                nrounds = 300,
                folds = folds,
                showsd = T,
                print_every_n = 10,
                verbose = 0,
                early_stopping_rounds = 20,
                maximize = F)

xgmod <- xgb.train(data = xgtrain,
                  params = cvparams,
                  nrounds = xgbcv$best_iteration,
                  print_every_n = 10,
                  verbose = 0,
                  watchlist = list(val= xgtest,train = xgtrain))

truth <- as.numeric(full_df$available[test_idx] == 't')

#roc plot
invisible(
  roc(truth, predict(xgmod, xgtest),
      smoothed = FALSE,
      ci=TRUE, ci.alpha=0.9, 
      plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, #grid=TRUE,
      print.auc=TRUE, show.thres=TRUE,
      quiet = TRUE)
)

#confusion plot
xgpred <- ifelse (predict(xgmod, xgtest) > 0.5,1,0)
conf_mat <- confusionMatrix(as.factor(xgpred), as.factor(truth))
fourfoldplot(conf_mat$table, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")

#importance plot
importance <- xgb.importance(feature_names = colnames(xgtrain), model = xgmod)
xgb.ggplot.importance(importance_matrix = importance)

#dependence plots
xgb.plot.shap(data = makeX(full_df[test_idx, features], sparse = TRUE), 
              model = xgmod, 
              top_n = 12, 
              n_col = 3, 
              plot_loess = TRUE)
```

